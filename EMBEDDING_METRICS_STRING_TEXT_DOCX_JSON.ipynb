{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9f06e9c",
   "metadata": {},
   "source": [
    "# Embedding Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1918de",
   "metadata": {},
   "source": [
    "### Importing libraries\n",
    "\n",
    "*here you can use any other gensim.model as you like*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0efe66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import docx\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd73732a",
   "metadata": {},
   "source": [
    "*use any vector embeddings, but GoogleNews-vectors-negative300.bin.gz is highly recommended*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f393eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the pre-trained Word2Vec model (you can also use your own trained model)\n",
    "# This example uses the Google News Word2Vec embeddings (large model)\n",
    "word2vec_path = 'GoogleNews-vectors-negative300.bin.gz'  # Update this with the correct path\n",
    "# Load the Word2Vec model\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e54f7bc",
   "metadata": {},
   "source": [
    "### Calculate here your scores: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e76171",
   "metadata": {},
   "source": [
    "#### If using a String: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f1a5a5c4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "chatbot_answer = \"\"\n",
    "reference_answer = \"\"\n",
    "\n",
    "similarity_score_greedy = calculate_similarity(chatbot_answer, reference_answer, model='greedy')\n",
    "print(\"Your Greedy Matching score: \", similarity_score_greedy)\n",
    "similarity_score_greedy = calculate_similarity(chatbot_answer, reference_answer, model='extrema')\n",
    "print(\"Your Vector Extrema score: \", similarity_score_greedy)\n",
    "similarity_score_greedy = calculate_similarity(chatbot_answer, reference_answer, model='average')\n",
    "print(\"Your Embedding Average score: \", similarity_score_greedy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc949e49",
   "metadata": {},
   "source": [
    "#### If using files: \n",
    "\n",
    "*you can also mix up docx, json or txt files. E.g., If you have you chatbot answers on jSon but your reference answer is on docx that still works.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5531abad",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_answer_files = \"\"\n",
    "reference_answer_files = \"\"\n",
    "\n",
    "similarity_score_greedy = calculate_similarity_from_files(chatbot_answer_files, reference_answer_files, model='greedy')\n",
    "print(\"Your Greedy Matching score: \", similarity_score_greedy)\n",
    "similarity_score_greedy = calculate_similarity_from_files(chatbot_answer_files, reference_answer_files, model='extrema')\n",
    "print(\"Your Vector Extrema score: \", similarity_score_greedy)\n",
    "similarity_score_greedy = calculate_similarity_from_files(chatbot_answer_files, reference_answer_files, model='average')\n",
    "print(\"Your Embedding Average score: \", similarity_score_greedy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e663a569",
   "metadata": {},
   "source": [
    "### Preprocessing, tokenising sentences and reading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9b74a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    # Tokenize and preprocess the sentence\n",
    "    return [token.lower() for token in sentence.split()]\n",
    "\n",
    "def read_text_from_file(filepath):\n",
    "    # Read text content from a text file\n",
    "    with open(filepath, 'r') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def read_text_from_docx(filepath):\n",
    "    # Read text content from a docx file\n",
    "    doc = docx.Document(filepath)\n",
    "    text = \"\"\n",
    "    for para in doc.paragraphs:\n",
    "        text += para.text + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def read_text_from_json(filepath, key):\n",
    "    # Read text content from a JSON file under a specific key\n",
    "    with open(filepath, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data.get(key, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1045ac",
   "metadata": {},
   "source": [
    "### Greedy Matching score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d19fc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_word_in_reference(token, reference_sentence, model):\n",
    "    # Find the word in the reference sentence that is closest in meaning to the given token\n",
    "    similarities = [(word, model.similarity(token, word)) for word in reference_sentence if word in model]\n",
    "    if similarities:\n",
    "        closest_word, _ = max(similarities, key=lambda x: x[1])\n",
    "        return closest_word\n",
    "    else:\n",
    "        # Return the token itself if no words in the reference sentence are present in the model\n",
    "        return token\n",
    "\n",
    "def sentence_to_vector_greedy(sentence, reference_sentence, model):\n",
    "    # Convert a sentence to a vector representation using word embeddings\n",
    "    vectors = [model[token] - model[find_closest_word_in_reference(token, reference_sentence, model)] for token in sentence if token in model]\n",
    "    if vectors:\n",
    "        # Take the sum of the word vectors to represent the sentence\n",
    "        sentence_vector = np.sum(vectors, axis=0)\n",
    "    else:\n",
    "        # Return zero vector if none of the tokens are present in the model\n",
    "        sentence_vector = np.zeros(model.vector_size)\n",
    "    return sentence_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f27d2a",
   "metadata": {},
   "source": [
    "### Vector Extrema score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94792cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_vector_extrema(sentence, model):\n",
    "    # Convert a sentence to a vector representation using word embeddings\n",
    "    vectors = [model[token] for token in sentence if token in model]\n",
    "    if vectors:\n",
    "        # Take the element-wise maximum and minimum of word vectors to represent the sentence\n",
    "        max_vector = np.max(vectors, axis=0)\n",
    "        min_vector = np.min(vectors, axis=0)\n",
    "        # Combine the maximum and minimum vectors (e.g., concatenation)\n",
    "        sentence_vector = np.concatenate([max_vector, min_vector])\n",
    "    else:\n",
    "        # Return zero vector if none of the tokens are present in the model\n",
    "        sentence_vector = np.zeros(model.vector_size * 2)  # Double the dimension for max and min vectors\n",
    "    return sentence_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76276485",
   "metadata": {},
   "source": [
    "### Embeddings Average score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c9ffb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_vector_average(sentence, model):\n",
    "    # Convert a sentence to a vector representation using word embeddings\n",
    "    vectors = [model[token] for token in sentence if token in model]\n",
    "    if vectors:\n",
    "        # Take the average of word vectors to represent the sentence\n",
    "        sentence_vector = np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        # Return zero vector if none of the tokens are present in the model\n",
    "        sentence_vector = np.zeros(model.vector_size)\n",
    "    return sentence_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4531ec",
   "metadata": {},
   "source": [
    "### Core function Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d746499",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(chatbot_answer, reference_answer, model='extrema'):\n",
    "    # Preprocess the sentences\n",
    "    chatbot_answer = preprocess_sentence(chatbot_answer)\n",
    "    reference_answer = preprocess_sentence(reference_answer)\n",
    "\n",
    "    if model == 'extrema':\n",
    "        # Convert sentences to vectors using Extrema method\n",
    "        chatbot_vector = sentence_to_vector_extrema(chatbot_answer, word2vec_model)\n",
    "        reference_vector = sentence_to_vector_extrema(reference_answer, word2vec_model)\n",
    "    elif model == 'average':\n",
    "        # Convert sentences to vectors using Average method\n",
    "        chatbot_vector = sentence_to_vector_average(chatbot_answer, word2vec_model)\n",
    "        reference_vector = sentence_to_vector_average(reference_answer, word2vec_model)\n",
    "    elif model == 'greedy':\n",
    "        # Convert sentences to vectors using Greedy Matching method\n",
    "        chatbot_vector = sentence_to_vector_greedy(chatbot_answer, reference_answer, word2vec_model)\n",
    "        reference_vector = sentence_to_vector_greedy(reference_answer, chatbot_answer, word2vec_model)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model. Choose 'extrema', 'average', or 'greedy'.\")\n",
    "\n",
    "    # Check if either vector is a zero vector\n",
    "    if np.all(chatbot_vector == 0) or np.all(reference_vector == 0):\n",
    "        similarity_score = 0.0\n",
    "    else:\n",
    "        # Calculate cosine similarity between the two vectors\n",
    "        similarity_score = np.dot(chatbot_vector, reference_vector) / (np.linalg.norm(chatbot_vector) * np.linalg.norm(reference_vector))\n",
    "    return similarity_score\n",
    "\n",
    "def calculate_similarity_from_files(chatbot_file, reference_file, model='extrema'):\n",
    "    # Read content from chatbot and reference files\n",
    "    if chatbot_file.endswith('.docx'):\n",
    "        chatbot_answer = read_text_from_docx(chatbot_file)\n",
    "    elif chatbot_file.endswith('.json'):\n",
    "        chatbot_answer = read_text_from_json(chatbot_file, \"chatbot\")\n",
    "    else:\n",
    "        chatbot_answer = read_text_from_file(chatbot_file)\n",
    "\n",
    "    if reference_file.endswith('.docx'):\n",
    "        reference_answer = read_text_from_docx(reference_file)\n",
    "    elif reference_file.endswith('.json'):\n",
    "        reference_answer = read_text_from_json(reference_file, \"reference\")\n",
    "    else:\n",
    "        reference_answer = read_text_from_file(reference_file)\n",
    "\n",
    "    return calculate_similarity(chatbot_answer, reference_answer, model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
